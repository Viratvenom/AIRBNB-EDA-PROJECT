{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "PH-0ReGfmX4f",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "3RnN4peoiCZX",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "hfrP0n_NmjLh",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "7SwLDUkz9eOZ",
        "uYnE1hNfvGyo",
        "PAyb90SyaMM5",
        "Apgm-TO3R1Nc",
        "UePMNsiESPB3",
        "Fjb1IsQkh3yE",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viratvenom/AIRBNB-EDA-PROJECT/blob/main/Copy_of_Sample_EDA_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Project Name** -\n",
        "#**Yes Bank Stock Closing Price Prediction**\n",
        "\n",
        "**Project Type -(EDA - Regression)**\n",
        "\n",
        "**Project by AMIT KUMAR SINGH**\n",
        "\n",
        "**Data Science Trainee at AlmaBetter**\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stockâ€™s closing price of the month.**"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.\n",
        "\n",
        "* There is a high correlation between the dependent and independent variables. This is a signal that our dependent variable is highly dependent on our features and can be predicted accurately from them.\n",
        "\n",
        "* We found that there is a rather high correlation between our independent variables. This multicollinearity is however unavoidable here as the dataset is very small.\n",
        "\n",
        "* We implemented several models on our dataset in order to be able to predict the closing price and found that all our models are performing remarkably well and Elastic Net regressor is the best performing model with Adjusted R2 score value of 0.9932 and scores well on all evaluation metrics.\n",
        "\n",
        "* All of the implemented models performed quite well on our data giving us the Adjusted R-square of over 99%.\n",
        "\n",
        "* We checked for presence of Heterodasceticity in our dataset by plotting the residuals against the Elastic Net model predicted value and found that there is no Heterodasceticity present. Our model is performing well on all data-points."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Your Business Objective?**"
      ],
      "metadata": {
        "id": "PH-0ReGfmX4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period."
      ],
      "metadata": {
        "id": "PhDvGCAqmjP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NkAXZc2pbAfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-yUsqN3q6E2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "working_path ='/content/drive/MyDrive/Colab Notebooks/data_YesBank_StockPrices.csv'\n",
        "df = pd.read_csv(working_path)"
      ],
      "metadata": {
        "id": "duCaEf_s6OD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "zXBreP2tb_YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample of dataset\n",
        "df.head().transpose()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **TIME SERIES DATA**\n",
        "\n",
        "A time series is a collection of data point against various time periods.\n",
        "IN the above data this is called BI-variate data sheet because YES bank stock market open and closed."
      ],
      "metadata": {
        "id": "rnu_SCZewQea"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape\n",
        "     "
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.High.value_counts()"
      ],
      "metadata": {
        "id": "rKWYj1u8aQkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information\n",
        "*  **Date :-** The date (Month and Year provided)\n",
        "*  **Open :-** The price of the stock at the beginning of a particular time period.\n",
        "*  **High :-**The Peak(Maximum) price at which a stock traded during the period.\n",
        "*  **Low :-**The Lowest price at which a stock traded during the period.\n",
        "*  **Close :-** The trading price at the end (in this case end of the month)."
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data pre-processing**:\n",
        "* Dealing with outliers\n",
        "\n",
        "* Handling Missing Values \n",
        "\n",
        "* Feature Scaling"
      ],
      "metadata": {
        "id": "hfrP0n_NmjLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Coun\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicate instances.\n",
        "df[df.duplicated()==True]"
      ],
      "metadata": {
        "id": "dKv6N8iMK1y1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the datatypes once more.\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "sP7q1uOnpbZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()\n",
        "     "
      ],
      "metadata": {
        "id": "4-XTjKIfMCWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **convert string object to datetime object**"
      ],
      "metadata": {
        "id": "Kf3GqScFLuK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# as we can see, Date column has the object datatype. \n",
        "df['Date']"
      ],
      "metadata": {
        "id": "EGyN34X7LJ7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets convert Date column to a proper datetime datatype.\n",
        "from datetime import datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'].apply(lambda x: datetime.strptime(x, '%d-%b'))) "
      ],
      "metadata": {
        "id": "2LJazW5kL5F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.set_index('Date', inplace=True)  "
      ],
      "metadata": {
        "id": "LhSJqggQdhMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head().transpose()"
      ],
      "metadata": {
        "id": "rhCBnv1htD5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?\n",
        "* We have a total of 185 entries & 5 colums.\n",
        "* No null values.\n",
        "* Date column  of 'object' datatype we have to convert it to 'datetime'.\n",
        "* check the structure of the datafram after dropping there is no any dupicates values / unwanted colums.\n",
        "* In the datafram we find only one object i.e \"DATE\" \n",
        "* P.I.I datasheet (Personal Identifiable Information)\n",
        "* Since we have a very small dataset to work with, dropping the outliers completely is not a good idea. So this is how we are going to leave them."
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core.fromnumeric import transpose\n",
        "# Dataset Describe\n",
        "df.describe().transpose()    #we need only numerical colums."
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 200)"
      ],
      "metadata": {
        "id": "kHM3dki5pnqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr().transpose()"
      ],
      "metadata": {
        "id": "E18tLaUOp0_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description \n",
        "* In height colums :\n",
        "  - difference between mean(116.104324)and median(11.24) equal to 104.864\n",
        "  huge differnce b/w them \n",
        "   - look at the distance b/w the length of left hand tail & right hand tail\n",
        "     [ min difference b/w 25% ] in first Quardtant i.e 11.24 - 36.14 = 24.9\n",
        "     so they are not given symmertical skew metrix"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **EDA**"
      ],
      "metadata": {
        "id": "7SwLDUkz9eOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visualise the relationship between each pair of variables using pair plots.\n",
        "import seaborn as sns\n",
        "yes_df = df.iloc[:,1:5]\n",
        "sns.pairplot (df,diag_kind = 'kde')         # estimate the possible denstity distrubution in mathamatical base observation on datasheat"
      ],
      "metadata": {
        "id": "1K0ApdZ-8I_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In diangol kind kde (kernel density kde ) in the from of square metrix\n",
        "compression b/w rows by rows\n",
        "* we see that first rows and first rows are same so on..\n",
        "* when we compare b/w colums vs columns its we get default result no any   information getting from them so we automatic give me histrogram plot with normal distrubution on the table .\n",
        "* now we see that close histrogram plot we get mean & median are equal now also see that its tails will be equall distrubted but we are appecting the normal distrubution but her is two gaussian and two normal curve overlapping each other this is called mixed of gaussian.\n",
        "so we assumed in algrothim are independent are distrubuted normally. gaussian mean distrubution.\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "independent_variables = df.columns.tolist()[:-1]\n",
        "dependent_variable = ['Close']\n",
        "\n",
        "print(independent_variables)\n",
        "print(dependent_variable)"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependent variable 'Close'\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.distplot(df['Close'],color=\"y\")"
      ],
      "metadata": {
        "id": "RmerTHNDMHMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependent variable 'Close'\n",
        "plt.figure(figsize=(7,7))                                       # LOG TRANSFORMATION used because in regression : its is not normal distrubution ( # other variabe and traget variable having N.D)\n",
        "sns.distplot(np.log10(df['Close']),color=\"y\")                    # bringing the larger value transfmatin"
      ],
      "metadata": {
        "id": "KX-3-JbU7Xoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can clearly see that these distributions are positively skewed. The mean and median are at significant distance from each other.\n",
        "\n",
        "So we need to transform them into something close to a Normal Distribution as our models give optimal results that way."
      ],
      "metadata": {
        "id": "m5qpbW3Vq9LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the dependent variable .\n",
        "plt.figure(figsize=(12,7))\n",
        "df['Close'].plot(color = 'r')\n",
        "plt.grid(which='major', linestyle='-', linewidth='0.5', color='green')\n",
        "plt.grid(which='minor', linestyle=':', linewidth='0.5', color='green')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Closing Price')\n",
        "plt.title('Closing Price with Date')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8JYeUHAcqsFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the stock price is rising up until 1901 when the fraud case involving Rana Kapoor happened after which the stock price has had a sharp decline. "
      ],
      "metadata": {
        "id": "UZ7lWANVqyzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = df.describe().columns\n",
        "numeric_features"
      ],
      "metadata": {
        "id": "UGD2qPhFF_Sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numeric_features[2:]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = np.log(df[col])\n",
        "    feature.hist(bins=50, ax = ax)\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)    \n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GlIiqbh7NPEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the independent variables against dependent variable close and also checking the correlation between them.\n",
        "for col in independent_variables:\n",
        "\n",
        "  fig = plt.figure(figsize=(9, 6))\n",
        "  ax = fig.gca()\n",
        "  feature = df[col]\n",
        "  label = df['Close']\n",
        "  correlation = feature.corr(label)        # calculating the correlation between dependent variable and independent features.\n",
        "  plt.scatter(x=feature, y=label)          # plotting dependent variables against independent features.  \n",
        "\n",
        "  # Setting the x,y labels and the title.\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('Close vs ' + col + '- correlation: ' + str(round((correlation),4)))\n",
        "\n",
        "  z = np.polyfit(df[col], df['Close'], 1)                                \n",
        "  y_hat = np.poly1d(z)(df[col])\n",
        "\n",
        "  plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-5uG3TH-Ia4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that all of our independent variables are highly correlated to the dependent variable.\n",
        "\n",
        "And the relationship between dependent and independent variables is linear in nature."
      ],
      "metadata": {
        "id": "Sg3Kqzd0rG52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ## Correlation\n",
        "plt.figure(figsize=(15,8))\n",
        "correlation = df.corr()\n",
        "sns.heatmap(abs(correlation), annot=True, cmap='coolwarm')"
      ],
      "metadata": {
        "id": "muDstwx_JtXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the heatmap above, we can clearly see that there is a very high correlation between each pair of features in our dataset. While it is desirable for the dependent variable to be highly correlated with independent variables, the independent varibles should ideally not have high correlation with one another.\n",
        "\n",
        "This causes a problem for us as high correlation among independent variables (multicollinearity) is a problem for our mod"
      ],
      "metadata": {
        "id": "wtv3dSh1rRgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IQR method\n",
        "we look at which data points are outside the whiskers. This method has the advantage, that it uses robust parameters for the calculation."
      ],
      "metadata": {
        "id": "uYnE1hNfvGyo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dealing with multicollinearity using VIF analysis.\n",
        "# Calculating VIF(Variation Inflation Factor) to see the correlation between independent variables\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ],
      "metadata": {
        "id": "-QwyxTjRubP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_vif(df[[i for i in df.describe().columns if i not in ['Date','Close']]])"
      ],
      "metadata": {
        "id": "0qJ1Ok77uw3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the values of VIF factor are very high. However since the dataset is so small and has just 3 independent features, multicollinearity is unavoidable here as any feature engineering will lead to loss of information."
      ],
      "metadata": {
        "id": "PrJDCqA6u6qy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating arrays of our input variable and label to feed the data to the model.\n",
        "# Create the data of independent variables\n",
        "X = np.log10(df[independent_variables]).values            # applying log transform on our independent variables.\n",
        "\n",
        "# Create the dependent variable data\n",
        "y = np.log10(df[dependent_variable]).values               # applying log transform on our dependent variable."
      ],
      "metadata": {
        "id": "Zkw7Tse0bW-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into a train and a test set. we do this using train test split.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)    "
      ],
      "metadata": {
        "id": "ZZOEVYADYbTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to avoid giving more importance to features with large values. This is achieved by normalization or standardization of the data."
      ],
      "metadata": {
        "id": "Pcb0YWY3rhf5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the values.\n",
        "X_train[0:10]"
      ],
      "metadata": {
        "id": "iy2z7kY1eOTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming data\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "-oSfEY_bdl5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split( X,y , test_size = 0.2, random_state = 0) \n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "yI3QMVsqPnmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Linear Regression**"
      ],
      "metadata": {
        "id": "PAyb90SyaMM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "reg = LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "62L50t7uQEG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the model.\n",
        "model_lr = LinearRegression()\n",
        "\n",
        "# Fitting the model on our train data.\n",
        "model_lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "eSScToCeajob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "23twCk_3QHZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "0yk-BBzwQLM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reg.coef_)"
      ],
      "metadata": {
        "id": "x2cuN5a-QPKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.intercept_"
      ],
      "metadata": {
        "id": "zWQPiAQAQR2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = reg.predict(X_test)"
      ],
      "metadata": {
        "id": "9FWak7JbQVPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred[0:10]"
      ],
      "metadata": {
        "id": "vKo-n-t8QcNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)"
      ],
      "metadata": {
        "id": "dc6keY66Qhva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "HisELCAGQmm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(10**(y_pred))\n",
        "plt.plot(np.array(10**(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "prVOhx_SQrX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we need to store our performance data for this model so that we can compare them with other models. "
      ],
      "metadata": {
        "id": "GbYySn9-rrfM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **2. Lasso Regression with cross validated regularization.**"
      ],
      "metadata": {
        "id": "DEr8vpuV-ZpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "lasso reduce all the way to zero thr0ugh the use 0f l1 regulazati0n , and contrast to the ridge regression\n",
        "our goal with be predic test score based on serval indepemdent regression  on the baseline ."
      ],
      "metadata": {
        "id": "dqto72lDZB2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000) # alpha is using beacause we want value of 0.0001 evenly spaced setting for the alpha of the outer argument\n",
        "lasso.fit(X_train, y_train)   #include on yes bank close prediction file which stands to cross validation to effect prcessing and refit updates the perameter\n",
        "\n"
      ],
      "metadata": {
        "id": "g5j7EjXFQ6L8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "SiK8ema9Q-pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "agJTImwfRCxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the coefficent value are also slighty different the only difference is the order was essentially set to zero this means that it all are useful variable for predicti0n"
      ],
      "metadata": {
        "id": "pOXDwlK6gSPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Cross validation\n",
        "from sklearn.model_selection import GridSearchCV              \n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "U1n2KBuURIWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we start be instatiate lasso with nan set t true it is impart to scale data when ding regeulized regression we set up our grid we incude the estimate and parameter grid and scoring the alpha is set using logspace between 15 to 0.001 and we want 21 \n",
        "\n",
        "After complating this we used the fit function the code below indictied the apprcite apha and expected scre if we run the m0de in alpha."
      ],
      "metadata": {
        "id": "MmyOUDqhdDlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "7LkPiNP_RcIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The alpha is set to zero, which is the same as a regression model. You can also see that the mean squared error is\n",
        "actually worse than in the baseline model. In the code below, we run the lasso model with the recommended alpha setting and\n",
        "print the results"
      ],
      "metadata": {
        "id": "q5Pyte8hhUxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso = lasso_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "Z8MSgW08RgdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(10**(y_pred_lasso))\n",
        "plt.plot(10**(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IR8Qd-nLRkpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred_lasso))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred_lasso))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "pv2dlZAxRqd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.Ridge Regression with cross validated regularization.** "
      ],
      "metadata": {
        "id": "Apgm-TO3R1Nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridge_regressor.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "nPFMkppsRvoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "K8qNuKwuR9LW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ridge_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "WKclydJuSE5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "znttPbeVSJ4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **Elastic-Net Regression with cross validation.**"
      ],
      "metadata": {
        "id": "UePMNsiESPB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5) #two variabe alpha and l1 "
      ],
      "metadata": {
        "id": "AUBZlPHhSRms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "p1u0UXmJSVyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "0v20yV7KSaYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_en = elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "yWW2fZjISfv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred_en))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred_en))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "c7Z8PMWASk7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elastic = ElasticNet()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':[0.3,0.4,0.5,0.6,0.7,0.8]}\n",
        "elastic_regressor = GridSearchCV(elastic, parameters, scoring='neg_mean_squared_error',cv=5)\n",
        "elastic_regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "H0BHccBqSpjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\n",
        "print(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)"
      ],
      "metadata": {
        "id": "hYOGTZ6BSubM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_elastic = elastic_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "fzv9uE95Sxm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE  = mean_squared_error(10**(y_test), 10**(y_pred_elastic))\n",
        "print(\"MSE :\" , MSE)\n",
        "\n",
        "RMSE = np.sqrt(MSE)\n",
        "print(\"RMSE :\" ,RMSE)\n",
        "\n",
        "r2 = r2_score(10**(y_test), 10**(y_pred_elastic))\n",
        "print(\"R2 :\" ,r2)\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(y_test), 10**(y_pred_elastic)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "Iv8qIqviS2ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Elastic Net as it scores the best in every single metric."
      ],
      "metadata": {
        "id": "wVzs2kt5sAkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets check for Heterodasticity. Homoscedasticity is an assumption in linear regression algorithm.\n",
        "# Homoscedasticity means that the model should perform well on all the datapoints.\n",
        "\n",
        "# Plotting the residuals(errors) against actual test data.\n",
        "residuals = 10**y_test - 10**y_pred.reshape(37,1)\n",
        "plt.scatter(10**y_test,residuals,c='red')\n",
        "plt.title('Actual Test data vs Residuals (Elastic Net)')"
      ],
      "metadata": {
        "id": "q7mYWIxPsSXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above graph, we have plotted the residuals against test set value for our actual test set values for our best performing model (Elastic Net Regressor).\n",
        "\n",
        "***As we can see, there is no discernable pattern here in the plot. The errors are similar for all datapoints and the model is performing equally well on all datapoints. So we can say that the assumption of Homoscedasticity is valid in this case.***  "
      ],
      "metadata": {
        "id": "Wa7uBLgVsoY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RandomForestClassifier**\n"
      ],
      "metadata": {
        "id": "KAWcZ4MsjNd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import r2_score as r2, mean_squared_error as mse\n",
        "import math\n",
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "metadata": {
        "id": "q4PfPmsAkAM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming data\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "IX2YM0IGjpde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=DecisionTreeRegressor(max_depth=5)\n",
        "df.fit(X_train, y_train)\n",
        "y_pred=df.predict(X_test)\n",
        "y_train_pred=df.predict(X_train)\n",
        "print('dt_regressor R^2: ', r2(y_test,y_pred))\n",
        "print('dt_regressor RMSE: ', math.sqrt(mse(y_test, y_pred)))\n",
        "print('dt_regressor RMSE: ', math.sqrt(mse(y_train, y_train_pred)))\n",
        "#print('dt_regressor RMSE: ',(np.sqrt(np.mean(np.square((y_test-y_pred)/y_test))))*100"
      ],
      "metadata": {
        "id": "mwY9P7dnjuuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_regressor=RandomForestRegressor(n_estimators =500,max_depth=5)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "y_pred=rf_regressor.predict(X_test)\n",
        "print('rf_regressor R^2: ', r2(y_test,y_pred))\n",
        "print('rf_regressor RMSE: ', math.sqrt(mse(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "vV0KhUp9jurA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg=LinearRegression()\n",
        "reg.fit(X_train,y_train)\n",
        "y_pred=reg.predict(X_test)\n",
        "print('R^2: ', r2(y_test,y_pred))\n",
        "print('RMSE: ',math.sqrt(mse(y_test,y_pred)))"
      ],
      "metadata": {
        "id": "KT0VOmY3kibx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusions :**\n",
        "---\n",
        "\n",
        "*   **Using data visualization on our target variable, we can clearly see the impact of 2018 fraud case involving Rana Kapoor as the stock prices decline dramatically during that period.**\n",
        "*   **After loading the dataset, we found that there are no null values in our dataset nor any duplicate data.**\n",
        "*   **There are some outliers in our features however this being a very small dataset, dropping those instances will lead to loss of information.**\n",
        "*   **We found that the distribution of all our variables is positively skewed. so we performed log transformation on them.**\n",
        "*   **There is a high correlation between the dependent and independent variables. This is a signal that our dependent variable is highly dependent on our features and can be predicted accurately from them.**\n",
        "*   **We found that there is a rather high correlation between our independent variables. This multicollinearity is however unavoidable here as the dataset is very small.**\n",
        "*   **We implemented several models on our dataset in order to be able to predict the closing price and found that all our models are performing remarkably well and *Elastic Net regressor is the best performing model with Adjusted R2 score value of 0.9932* and scores well on all evaluation metrics.** \n",
        "*   **All of the implemented models performed quite well on our data giving us the Adjusted R-square of over 99%.**\n",
        "*   **We checked for presence of Heterodasceticity in our dataset by plotting the residuals against the Elastic Net model predicted value and found that there is no Heterodasceticity present. Our model is performing well on all data-points.**\n",
        "*   **With our model making predictions with such high accuracy, we can confidently deploy this model for further predictive tasks using future data.** \n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your EDA Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}